# ğŸš€ Ramalama Kubernetes - Easy LLM Deployment Made Simple

[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Kubernetes](https://img.shields.io/badge/kubernetes-%23326ce5.svg?style=flat&logo=kubernetes&logoColor=white)](https://kubernetes.io)
[![OpenShift](https://img.shields.io/badge/OpenShift-EE0000?style=flat&logo=redhatopenshift&logoColor=white)](https://openshift.com)

> **Deploy powerful Language Models (LLMs) in Kubernetes with just a few commands!**

## What is this?

[Ramalama](https://github.com/containers/ramalama) with Kubernetes makes it incredibly easy to run your own ChatGPT-like AI models in Podman, Kubernetes or OpenShift. Whether you're a developer, DevOps engineer, or AI enthusiast, this project helps you:

- **Quick Setup** - One command deployment of AI Models
- **Use familiar tools** - Works with Podman, Kubernetes, and standard GitOps workflows
- **Kubernetes ready** - Includes monitoring, scaling, and security best practices
- **Model variety** - Support for multiple LLM models and sizes
- **Enterprise grade** - Built for OpenShift with proper RBAC and security policies

## Features

### **Architecture**
```mermaid
graph LR
    A[ğŸ“ Git Repo] --> B[ğŸ”„ Build Modelcars]
    B --> C[â˜¸ï¸ Podman or Kubernetes]
    C --> D[ğŸ¤– LLM Model serving OpenAI-like API]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
```

- **Declarative Deployments** - Everything as code with Kustomize and GitHub Actions for Modelcars
- **Security First** - SELinux, Kubernetes security standards and RBAC
- **Monitoring Ready** - Prometheus metrics and health checks

### ğŸ­ **Multiple Model Support**

| Model | Size | Status |
|-------|------|--------|
| **Qwen 3 1.7B**       | Small   | âœ… Ready |
| **Gemma 3n E4B**      | Small   | âœ… Ready |
| **Qwen 3 4B**         | Medium  | âœ… Ready |
| **Qwen 3 30B**        | Large   | âœ… Ready |
| **DeepSeek R1 Qwen3** | 8B      | âœ… Ready |
| **Custom**            | Any!    | âœ… Ready |

### ğŸ” **Security Features**

- **Pod Security Standards** - Restricted policies enforced
- **Non-root execution** - All containers run as non-root user
- **RBAC** - Role-based access control
- **Network Policies** - Micro-segmentation ready
- **Security Context** - Dropped capabilities and seccomp

## ğŸš€ Quick Start

### Prerequisites

![Kubernetes](https://img.shields.io/badge/Kubernetes-1.24+-blue?logo=kubernetes)
![OpenShift](https://img.shields.io/badge/OpenShift-4.15+-red?logo=redhatopenshift)
![Podman](https://img.shields.io/badge/Podman-4.0+-purple?logo=podman)

- **Kubernetes/OpenShift cluster** with admin access
- **Container runtime** (Podman recommended, Docker works if you insist)
- **kubectl or oc CLI** installed & configured

### 1ï¸âƒ£ Clone and Explore

```bash
git clone https://github.com/kush-gupt/ramalama-k8s.git
cd ramalama-k8s
```

### 2ï¸âƒ£ Deploy Your First Model

#### **Single Model with OpenShift (Recommended)**
```bash
# ğŸ—ï¸ Create namespace with:
oc apply -f k8s/models/ramalama-namespace.yaml
# Or this:
oc create namespace ramalama

# ğŸš€ Deploy Qwen 3 1B model on CPU 
oc apply -k k8s/models/qwen3-1b

# âœ… Verify deployment
oc get pods -l model=qwen3-1b -n ramalama
```

#### **Or with Podman on Linux directly**
```bash
# At this point, you're better off using Ramalama directly if you just want inference on Linux:
ramalama serve hf://unsloth/Qwen3-1.7B-GGUF/Qwen3-1.7B-UD-Q4_K_XL.gguf

# But for directly testing the images built here:
podman run -p 8080:8080 ghcr.io/kush-gupt/qwen3-1b-ramalama /usr/bin/llama-server --port 8080 --model /mnt/models/Qwen3-1.7B-UD-Q4_K_XL.gguf/Qwen3-1.7B-UD-Q4_K_XL.gguf
```

### 3ï¸âƒ£ Test Your Model

```bash
# ğŸ§ª Deploy a test pod in the cluster to test service connectivity
oc run model-test --image=curlimages/curl:8.10.1 --rm -i --tty -n ramalama -- sh

# ğŸ’¬ Inside the test pod, test the chat API (replace 'qwen3-1b' with your model name)
curl -X POST http://qwen3-1b-ramalama-service.ramalama.svc.cluster.local:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "default",
    "messages": [{"role": "user", "content": "Hello! How are you?"}],
    "temperature": 0.7
  }'

# ğŸ” Test model availability
curl http://qwen3-1b-ramalama-service.ramalama.svc.cluster.local:8080/v1/models

# ğŸ“‹ Exit the test pod (it will be automatically deleted due to --rm flag)
exit
```

## **OpenShift Lightspeed Integration**

![OpenShift Lightspeed](https://img.shields.io/badge/OpenShift%20Lightspeed-Ready-brightgreen?logo=redhatopenshift)

Turn your deployed models into an AI-powered OpenShift assistant!

### ğŸ“‹ **Prerequisites**
```bash
# âœ… Ensure you have at least one model running first
oc get pods -n ramalama

# ğŸš€ If no project or models are deployed, deploy them first:
oc apply -f k8s/models/ramalama-namespace.yaml
oc apply -k k8s/models/qwen3-4b
```

### ğŸ¯ **Deployment Options**

#### **Option 1: Direct Kustomize (Two-Step Process, reliable and minimalist)**
Due to operator timing dependencies, direct deployment requires two steps:

```bash
# ğŸ”§ Step 1: Install operator and create CRDs
oc apply -k k8s/lightspeed/base/operator-only

# â³ Step 2: Wait for operator to be ready (this creates the required CRDs)
oc wait --for=condition=Available deployment -l app.kubernetes.io/created-by=lightspeed-operator -n openshift-lightspeed --timeout=100s

# ğŸ¯ Step 3: Link lightspeed to the ramalama service
oc apply -k k8s/lightspeed/overlays/qwen3-4b
```

#### **Option 2: GitOps Deployment (If you have it installed)**
```bash
# ğŸ”¥ Deploy with ArgoCD - single command that should handle timing automatically
oc apply -f k8s/lightspeed/argocd/application-qwen3-4b.yaml

# âœ… Monitor deployment
oc get applications -n openshift-gitops | grep lightspeed
```

### **What You Get**
- **AI Assistant** integrated into OpenShift console
- **YAML Generation** - "Create a deployment for nginx"  
- **Troubleshooting** - "Why is my pod failing?"
- **Best Practices** - Expert OpenShift guidance
- **Auto-discovery** - Automatically connects to your deployed models

### **Verification**
```bash
# âœ… Check all components are running
oc get pods -n openshift-lightspeed
oc get olsconfig cluster -n openshift-lightspeed
```

[ğŸ“– **Detailed Lightspeed Setup Guide**](k8s/lightspeed/README.md)

## ğŸ—ï¸ **Architecture Overview**

```mermaid
graph TB
    subgraph "ğŸš€ GitOps Layer"
        G[ğŸ“ Git Repository]
        A[ğŸ”„ ArgoCD/OpenShift GitOps]
    end
    
    subgraph "â˜¸ï¸ Kubernetes Cluster"
        subgraph "ğŸ¤– ramalama namespace"
            M1[ğŸ“¦ qwen3-1b-deployment]
            M2[ğŸ“¦ qwen3-4b-deployment] 
            M3[ğŸ“¦ qwen3-30b-deployment]
            M4[ğŸ“¦ deepseek-r1-deployment]
        end
        
        subgraph "ğŸ¯ openshift-lightspeed namespace"
            LS[ğŸ¤– Lightspeed Assistant]
        end
    end
    
    subgraph "ğŸ‘¨â€ğŸ’» User Interfaces"
        CLI[ğŸ–¥ï¸ kubectl/oc CLI]
        WEB[ğŸŒ OpenShift Console]
        API[ğŸ”Œ REST APIs]
    end
    
    G --> A
    A --> M1
    A --> M2  
    A --> M3
    A --> M4
    A --> LS
    
    LS -.->|ğŸ”— Connects to| M2
    LS -.->|ğŸ”— Connects to| M3
    LS -.->|ğŸ”— Connects to| M4
    
    CLI --> M1
    CLI --> M2
    WEB --> LS
    API --> M1
    API --> M2
    
    style G fill:#e1f5fe
    style A fill:#f3e5f5
    style M1 fill:#e8f5e8
    style M2 fill:#e8f5e8
    style M3 fill:#e8f5e8  
    style M4 fill:#e8f5e8
    style LS fill:#fff3e0
```

## **Model Management**

### **Add New Models**

![Add Model](https://img.shields.io/badge/Script-add--model.sh-blue?logo=gnu-bash)

```bash
# ğŸ¯ Interactive mode
./scripts/add-model.sh --interactive

# ğŸš€ Command line mode with Lightspeed generation w/llama-7b as an example
./scripts/add-model.sh \
  --name "llama-7b" \
  --description "Llama 7B Chat model" \
  --model-gguf-url "hf://microsoft/Llama-7b-gguf" \
  --model-file "/mnt/models/llama-7b.gguf/llama-7b.gguf" \
  --create-lightspeed-overlay
```

**Auto-Generated Files:**
- ğŸ“¦ `containerfiles/Containerfile-llama-7b`
- â˜¸ï¸ `k8s/models/llama-7b/kustomization.yaml`
- ğŸ¯ `k8s/lightspeed/overlays/llama-7b/kustomization.yaml`
- ğŸ¤– `k8s/lightspeed/overlays/llama-7b/olsconfig.yaml`
- ğŸ“– `k8s/lightspeed/overlays/llama-7b/README.md`
- âš™ï¸ `models/llama-7b.conf`

### **List Models**
```bash
./scripts/list-models.sh
```

### **Remove Models**
```bash
./scripts/remove-model.sh llama-7b --force
```

## **Configuration**

### **Model Parameters**
```yaml
# Example model configuration
configMapGenerator:
- name: ramalama-config
  literals:
  - CTX_SIZE=20048        # ğŸ§  Context window size
  - THREADS=14            # ğŸ”„ CPU threads
  - TEMP=0.7              # ğŸŒ¡ï¸ Temperature (creativity)
  - TOP_K=40              # ğŸ¯ Top-K sampling
  - TOP_P=0.9             # ğŸ“Š Top-P sampling
```

### **Environment Variables**
```bash
# ğŸšª API Configuration
PORT=8080
HOST=0.0.0.0

# ğŸ§  Model Settings  
MODEL_FILE=/mnt/models/model.gguf
CTX_SIZE=4096
THREADS=14

# ï¿½ï¿½ Server Options
LOG_COLORS=true
NO_WARMUP=false
JINJA=true
```

## ğŸ¤ **Contributing**

![Contributors](https://img.shields.io/badge/Contributors-Welcome-brightgreen?logo=github)

1. **ğŸ´ Fork** the repository
2. **ğŸŒ¿ Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **ğŸ’¾ Commit** your changes (`git commit -m 'Add amazing feature'`)
4. **ğŸ“¤ Push** to the branch (`git push origin feature/amazing-feature`)
5. **ğŸ”„ Open** a Pull Request

### **Development Setup**
```bash
# ğŸ  Local development
./scripts/build-script.sh
podman build -f containerfiles/Containerfile-min .

# ğŸ§ª Test locally
./scripts/llama-server.sh
```

## ğŸ“š **Documentation**

- **[Kubernetes Deployment Guide](k8s/README.md)**
- **[OpenShift Lightspeed Integration](k8s/lightspeed/README.md)**  
- **[Model Management](MODELS.md)**
- **[GitOps Setup](OPENSHIFT_GITOPS.md)**

## ğŸ“œ **License**

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™‹ **Support**

- **ğŸ› Issues**: [GitHub Issues](https://github.com/kush-gupt/ramalama-k8s/issues)
- **ğŸ’¬ Discussions**: [GitHub Discussions](https://github.com/kush-gupt/ramalama-k8s/discussions)
---

**ğŸ‰ Ready to deploy your own AI models?**

[![Get Started](https://img.shields.io/badge/Get%20Started-Now-success?style=for-the-badge&logo=rocket)](k8s/README.md)
[![OpenShift Lightspeed](https://img.shields.io/badge/OpenShift%20Lightspeed-Deploy-red?style=for-the-badge&logo=redhatopenshift)](k8s/lightspeed/README.md)

*Made for the open source community*
