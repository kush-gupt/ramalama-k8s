# ğŸš€ Ramalama Kubernetes - Easy LLM Deployment Made Simple

[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Red Hat](https://img.shields.io/badge/Red%20Hat-EE0000?style=flat&logo=redhat&logoColor=white)](https://redhat.com)
[![Kubernetes](https://img.shields.io/badge/kubernetes-%23326ce5.svg?style=flat&logo=kubernetes&logoColor=white)](https://kubernetes.io)
[![OpenShift](https://img.shields.io/badge/OpenShift-EE0000?style=flat&logo=redhatopenshift&logoColor=white)](https://openshift.com)

> **Deploy powerful Language Models (LLMs) in Kubernetes with just a few commands!**

## ğŸ¯ What is this?

Ramalama with Kubernetes makes it incredibly easy to run your own ChatGPT-like AI models in Kubernetes or OpenShift. Whether you're a developer, DevOps engineer, or AI enthusiast, this project helps you:

- âš¡ **Get started quickly** - Deploy AI models in minutes, not hours
- ğŸ”§ **Use familiar tools** - Works with Docker, Kubernetes, and standard GitOps workflows
- ğŸ¤– **Production ready** - Includes monitoring, scaling, and security best practices
- ğŸ“¦ **Model variety** - Support for multiple LLM models and sizes
- ğŸ›¡ï¸ **Enterprise grade** - Built for OpenShift with proper RBAC and security policies

## âœ¨ Features

### ğŸ—ï¸ **GitOps Architecture**
```mermaid
graph LR
    A[ğŸ“ Git Repo] --> B[ğŸ”„ ArgoCD]
    B --> C[â˜¸ï¸ Kubernetes]
    C --> D[ğŸ¤– LLM Models]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
```

- **ğŸ”„ Declarative Deployments** - Everything as code with Kustomize
- **ğŸ“ˆ Auto-scaling** - Horizontal pod autoscaling based on demand  
- **ğŸ”’ Security First** - Pod security standards and RBAC
- **ğŸ“Š Monitoring Ready** - Prometheus metrics and health checks

### ğŸ­ **Multiple Model Support**

| Model | Size | Use Case | Status |
|-------|------|----------|---------|
| **Qwen 3 1.7B** | Small | ğŸ’¬ Chat, Q&A | âœ… Ready |
| **Qwen 3 4B** | Medium | ğŸ’¼ Business tasks | âœ… Ready |  
| **Qwen 3 30B** | Large | ğŸ§  Complex reasoning | âœ… Ready |
| **DeepSeek R1** | 8B | ğŸ”¬ Research tasks | âœ… Ready |

### ğŸ› ï¸ **Developer Experience**

- **âš¡ Quick Setup** - One command deployment
- **ğŸ”§ Easy Configuration** - YAML-based model definitions
- **ğŸ“± API Compatible** - OpenAI-compatible endpoints
- **ğŸ” Auto-discovery** - Automatic service discovery for OpenShift Lightspeed

## ğŸš€ Quick Start

### Prerequisites

![Kubernetes](https://img.shields.io/badge/Kubernetes-1.24+-blue?logo=kubernetes)
![OpenShift](https://img.shields.io/badge/OpenShift-4.15+-red?logo=redhatopenshift)
![Podman](https://img.shields.io/badge/Podman-4.0+-purple?logo=podman)

- **â˜¸ï¸ Kubernetes/OpenShift cluster** with admin access
- **ğŸ³ Container runtime** (Podman recommended, Docker works)
- **ğŸ”§ kubectl/oc CLI** configured

### 1ï¸âƒ£ Clone and Explore

```bash
git clone https://github.com/kush-gupt/ramalama-k8s.git
cd ramalama-k8s

# ğŸ“‹ List available models
./scripts/list-models.sh
```

### 2ï¸âƒ£ Deploy Your First Model

#### ğŸ¯ **Single Model (Recommended)**
```bash
# ğŸ—ï¸ Create namespace
kubectl apply -f k8s/models/ramalama-namespace.yaml

# ğŸš€ Deploy Qwen 3 4B model  
kubectl apply -k k8s/models/qwen3-4b

# âœ… Verify deployment
kubectl get pods -l model=qwen3-4b -n ramalama
```

#### ğŸŒ **Environment-Based Deployment**
```bash
# ğŸ§ª Development environment
kubectl apply -k k8s/overlays/dev

# ğŸ­ Production environment  
kubectl apply -k k8s/overlays/production
```

### 3ï¸âƒ£ Test Your Model

```bash
# ğŸ”— Port forward to access the API
kubectl port-forward -n ramalama svc/qwen3-4b-ramalama-service 8080:8080

# ğŸ’¬ Test the chat API
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-4b-model",
    "messages": [{"role": "user", "content": "Hello! How are you?"}],
    "temperature": 0.7
  }'
```

## ğŸ›ï¸ **OpenShift Lightspeed Integration**

![OpenShift Lightspeed](https://img.shields.io/badge/OpenShift%20Lightspeed-Ready-brightgreen?logo=redhatopenshift)

Turn your deployed models into an AI-powered OpenShift assistant!

### ğŸ‰ **One-Command Setup**
```bash
# ğŸ”¥ Deploy with GitOps
kubectl apply -f k8s/lightspeed/argocd/application-qwen3-4b.yaml

# ğŸ”§ Or deploy directly
kubectl apply -k k8s/lightspeed/overlays/qwen3-4b
```

### ğŸ’¡ **What You Get**
- **ğŸ¤– AI Assistant** integrated into OpenShift console
- **ğŸ“ YAML Generation** - "Create a deployment for nginx"  
- **ğŸ” Troubleshooting** - "Why is my pod failing?"
- **ğŸ’¡ Best Practices** - Expert OpenShift guidance

[ğŸ“– **Detailed Lightspeed Setup Guide**](k8s/lightspeed/README.md)

## ğŸ—ï¸ **Architecture Overview**

```mermaid
graph TB
    subgraph "ğŸš€ GitOps Layer"
        G[ğŸ“ Git Repository]
        A[ğŸ”„ ArgoCD/OpenShift GitOps]
    end
    
    subgraph "â˜¸ï¸ Kubernetes Cluster"
        subgraph "ğŸ¤– ramalama namespace"
            M1[ğŸ“¦ qwen3-1b-deployment]
            M2[ğŸ“¦ qwen3-4b-deployment] 
            M3[ğŸ“¦ qwen3-30b-deployment]
            M4[ğŸ“¦ deepseek-r1-deployment]
        end
        
        subgraph "ğŸ¯ openshift-lightspeed namespace"
            LS[ğŸ¤– Lightspeed Assistant]
        end
    end
    
    subgraph "ğŸ‘¨â€ğŸ’» User Interfaces"
        CLI[ğŸ–¥ï¸ kubectl/oc CLI]
        WEB[ğŸŒ OpenShift Console]
        API[ğŸ”Œ REST APIs]
    end
    
    G --> A
    A --> M1
    A --> M2  
    A --> M3
    A --> M4
    A --> LS
    
    LS -.->|ğŸ”— Connects to| M2
    LS -.->|ğŸ”— Connects to| M3
    LS -.->|ğŸ”— Connects to| M4
    
    CLI --> M1
    CLI --> M2
    WEB --> LS
    API --> M1
    API --> M2
    
    style G fill:#e1f5fe
    style A fill:#f3e5f5
    style M1 fill:#e8f5e8
    style M2 fill:#e8f5e8
    style M3 fill:#e8f5e8  
    style M4 fill:#e8f5e8
    style LS fill:#fff3e0
```

## ğŸ› ï¸ **Model Management**

### â• **Add New Models**

![Add Model](https://img.shields.io/badge/Script-add--model.sh-blue?logo=gnu-bash)

```bash
# ğŸ¯ Interactive mode
./scripts/add-model.sh --interactive

# ğŸš€ Command line mode
./scripts/add-model.sh \
  --name "llama-7b" \
  --description "Llama 7B Chat model" \
  --model-gguf-url "hf://microsoft/Llama-7b-gguf" \
  --model-file "/mnt/models/llama-7b.gguf/llama-7b.gguf" \
  --create-lightspeed-overlay
```

### ğŸ“‹ **List Models**
```bash
./scripts/list-models.sh
```

### ğŸ—‘ï¸ **Remove Models**
```bash
./scripts/remove-model.sh llama-7b --force
```

## ğŸ­ **Production Deployment**

### ğŸ¯ **With OpenShift GitOps**

![GitOps](https://img.shields.io/badge/GitOps-Enabled-green?logo=argo)

```bash
# ğŸ”„ Single model application
kubectl apply -f k8s/argocd/application-example.yaml

# ğŸŒ Multi-environment ApplicationSet
kubectl apply -f k8s/argocd/applicationset-example.yaml
```

### ğŸ“Š **Resource Requirements**

| Model Size | Memory | CPU | GPU | Storage |
|------------|--------|-----|-----|---------|
| **1.7B** | 4Gi | 2 cores | Optional | 10Gi |
| **4B** | 8Gi | 4 cores | Optional | 20Gi |
| **8B** | 16Gi | 8 cores | Recommended | 40Gi |
| **30B** | 32Gi | 16 cores | Required | 80Gi |

### ğŸ” **Security Features**

- **ğŸ›¡ï¸ Pod Security Standards** - Restricted policies enforced
- **ğŸ‘¤ Non-root execution** - All containers run as non-root user
- **ğŸ”’ RBAC** - Role-based access control
- **ğŸ” Network Policies** - Micro-segmentation ready
- **ğŸ“‹ Security Context** - Dropped capabilities and seccomp

## ğŸ“ **Project Structure**

```
ramalama-k8s/
â”œâ”€â”€ ğŸ“¦ containerfiles/           # Container build definitions
â”œâ”€â”€ â˜¸ï¸ k8s/                      # Kubernetes manifests
â”‚   â”œâ”€â”€ ğŸ—ï¸ base/                 # Base Kustomize resources  
â”‚   â”œâ”€â”€ ğŸ­ models/               # Model-specific configs
â”‚   â”œâ”€â”€ ğŸŒ overlays/             # Environment overlays
â”‚   â”œâ”€â”€ ğŸ¯ lightspeed/           # OpenShift Lightspeed integration
â”‚   â””â”€â”€ ğŸ”„ argocd/               # GitOps applications
â”œâ”€â”€ ğŸ¤– models/                   # Model configurations
â”œâ”€â”€ ğŸ› ï¸ scripts/                  # Management scripts
â””â”€â”€ ğŸ“š docs/                     # Documentation
```

## ğŸ”§ **Configuration**

### ğŸ›ï¸ **Model Parameters**
```yaml
# Example model configuration
configMapGenerator:
- name: ramalama-config
  literals:
  - CTX_SIZE=20048        # ğŸ§  Context window size
  - THREADS=14            # ğŸ”„ CPU threads
  - TEMP=0.7              # ğŸŒ¡ï¸ Temperature (creativity)
  - TOP_K=40              # ğŸ¯ Top-K sampling
  - TOP_P=0.9             # ğŸ“Š Top-P sampling
```

### ğŸŒ **Environment Variables**
```bash
# ğŸšª API Configuration
PORT=8080
HOST=0.0.0.0

# ğŸ§  Model Settings  
MODEL_FILE=/mnt/models/model.gguf
CTX_SIZE=4096
THREADS=14

# ï¿½ï¿½ Server Options
LOG_COLORS=true
NO_WARMUP=false
JINJA=true
```

## ğŸ” **Monitoring & Troubleshooting**

### ğŸ“Š **Health Checks**
```bash
# âœ… Check model health
kubectl get pods -l app.kubernetes.io/name=ramalama -n ramalama

# ğŸ“ View logs
kubectl logs -l model=qwen3-4b -n ramalama --tail=100

# ğŸ”§ Debug service connectivity
kubectl port-forward -n ramalama svc/qwen3-4b-ramalama-service 8080:8080
curl http://localhost:8080/v1/models
```

### ğŸ†˜ **Common Issues**

| Issue | Solution |
|-------|----------|
| ğŸš« Pod not starting | Check resource limits and node capacity |
| ğŸ”Œ API not responding | Verify port-forward and service endpoints |  
| ğŸŒ Slow responses | Increase CPU/memory or enable GPU |
| ğŸ“¦ Image pull errors | Check registry credentials and image tags |

## ğŸ¤ **Contributing**

![Contributors](https://img.shields.io/badge/Contributors-Welcome-brightgreen?logo=github)

1. **ğŸ´ Fork** the repository
2. **ğŸŒ¿ Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **ğŸ’¾ Commit** your changes (`git commit -m 'Add amazing feature'`)
4. **ğŸ“¤ Push** to the branch (`git push origin feature/amazing-feature`)
5. **ğŸ”„ Open** a Pull Request

### ğŸ¯ **Development Setup**
```bash
# ğŸ  Local development
./scripts/build-script.sh
podman build -f containerfiles/Containerfile-min .

# ğŸ§ª Test locally
./scripts/llama-server.sh
```

## ğŸ“š **Documentation**

- **ğŸ“– [Kubernetes Deployment Guide](k8s/README.md)**
- **ğŸ¯ [OpenShift Lightspeed Integration](k8s/lightspeed/README.md)**  
- **ğŸ—ï¸ [Model Management](MODELS.md)**
- **ğŸ”„ [GitOps Setup](OPENSHIFT_GITOPS.md)**

## ğŸ“œ **License**

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™‹ **Support**

- **ğŸ› Issues**: [GitHub Issues](https://github.com/kush-gupt/ramalama-k8s/issues)
- **ğŸ’¬ Discussions**: [GitHub Discussions](https://github.com/kush-gupt/ramalama-k8s/discussions)
- **ğŸ“§ Email**: [Support](mailto:support@example.com)

---

**ğŸ‰ Ready to deploy your own AI models?**

[![Get Started](https://img.shields.io/badge/Get%20Started-Now-success?style=for-the-badge&logo=rocket)](k8s/README.md)
[![OpenShift Lightspeed](https://img.shields.io/badge/OpenShift%20Lightspeed-Deploy-red?style=for-the-badge&logo=redhatopenshift)](k8s/lightspeed/README.md)

*Made with â¤ï¸ for the open source community*
