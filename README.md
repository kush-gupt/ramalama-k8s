# ğŸš€ Ramalama Kubernetes - Easy LLM Deployment Made Simple

[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Kubernetes](https://img.shields.io/badge/kubernetes-%23326ce5.svg?style=flat&logo=kubernetes&logoColor=white)](https://kubernetes.io)
[![OpenShift](https://img.shields.io/badge/OpenShift-EE0000?style=flat&logo=redhatopenshift&logoColor=white)](https://openshift.com)

> **Deploy powerful Language Models (LLMs) in Kubernetes with just a few commands!**

## What is this?

Ramalama with Kubernetes makes it incredibly easy to run your own ChatGPT-like AI models in Kubernetes or OpenShift. Whether you're a developer, DevOps engineer, or AI enthusiast, this project helps you:

- **Get started quickly** - Deploy AI models in minutes, not hours
- **Use familiar tools** - Works with Docker, Kubernetes, and standard GitOps workflows
- **Production ready** - Includes monitoring, scaling, and security best practices
- **Model variety** - Support for multiple LLM models and sizes
- **Enterprise grade** - Built for OpenShift with proper RBAC and security policies

## Features

### **GitOps Architecture**
```mermaid
graph LR
    A[ğŸ“ Git Repo] --> B[ğŸ”„ ArgoCD]
    B --> C[â˜¸ï¸ Kubernetes]
    C --> D[ğŸ¤– LLM Models]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
```

- **Declarative Deployments** - Everything as code with Kustomize
- **Auto-scaling** - Horizontal pod autoscaling based on demand  
- **Security First** - Pod security standards and RBAC
- **Monitoring Ready** - Prometheus metrics and health checks

### ğŸ­ **Multiple Model Support**

| Model | Size | Use Case | Status |
|-------|------|----------|---------|
| **Qwen 3 1.7B** | Small | ğŸ’¬ Chat, Q&A | âœ… Ready |
| **Qwen 3 4B** | Medium | ğŸ’¼ Business tasks | âœ… Ready |  
| **Qwen 3 30B** | Large | ğŸ§  Complex reasoning | âœ… Ready |
| **DeepSeek R1** | 8B | ğŸ”¬ Research tasks | âœ… Ready |

### **Developer Experience**

- **Quick Setup** - One command deployment
- **Easy Configuration** - YAML-based model definitions
- **API Compatible** - OpenAI-compatible endpoints
- **Auto-discovery** - Automatic service discovery for OpenShift Lightspeed

## ğŸš€ Quick Start

### Prerequisites

![Kubernetes](https://img.shields.io/badge/Kubernetes-1.24+-blue?logo=kubernetes)
![OpenShift](https://img.shields.io/badge/OpenShift-4.15+-red?logo=redhatopenshift)
![Podman](https://img.shields.io/badge/Podman-4.0+-purple?logo=podman)

- **Kubernetes/OpenShift cluster** with admin access
- **Container runtime** (Podman recommended, Docker works if you insist)
- **kubectl/oc CLI** configured

### 1ï¸âƒ£ Clone and Explore

```bash
git clone https://github.com/kush-gupt/ramalama-k8s.git
cd ramalama-k8s
```

### 2ï¸âƒ£ Deploy Your First Model

#### **Single Model (Recommended)**
```bash
# ğŸ—ï¸ Create namespace
oc apply -f k8s/models/ramalama-namespace.yaml

# ğŸš€ Deploy Qwen 3 1B model on CPU 
oc apply -k k8s/models/qwen3-1b

# âœ… Verify deployment
oc get pods -l model=qwen3-4b -n ramalama
```

#### **Environment-Based Deployment**
```bash
# ğŸ§ª Development environment
oc apply -k k8s/overlays/dev

# ğŸ­ Production environment  
oc apply -k k8s/overlays/production
```

### 3ï¸âƒ£ Test Your Model

```bash
# ğŸ”— Port forward to access the API
oc port-forward -n ramalama svc/qwen3-4b-ramalama-service 8080:8080

# ğŸ’¬ Test the chat API
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-4b-model",
    "messages": [{"role": "user", "content": "Hello! How are you?"}],
    "temperature": 0.7
  }'
```

## ğŸ›ï¸ **OpenShift Lightspeed Integration**

![OpenShift Lightspeed](https://img.shields.io/badge/OpenShift%20Lightspeed-Ready-brightgreen?logo=redhatopenshift)

Turn your deployed models into an AI-powered OpenShift assistant!

### ğŸ“‹ **Prerequisites**
```bash
# âœ… Ensure you have at least one model running first
oc get pods -l app.kubernetes.io/name=ramalama -n ramalama

# ğŸš€ If no project or models are deployed, deploy one first:
oc apply -f k8s/models/ramalama-namespace.yaml
oc apply -k k8s/models/qwen3-4b
```

### ğŸ¯ **Deployment Options**

#### **Option 1: GitOps Deployment (Recommended)**
```bash
# ğŸ”¥ Deploy with ArgoCD - single command handles timing automatically
oc apply -f k8s/lightspeed/argocd/application-qwen3-4b.yaml

# âœ… Monitor deployment
oc get applications -n openshift-gitops | grep lightspeed
```

#### **Option 2: Direct Kustomize (Two-Step Process)**
Due to operator timing dependencies, direct deployment requires two steps:

```bash
# ğŸ”§ Step 1: Install operator and create CRDs
oc apply -k k8s/lightspeed/base/operator-only

# â³ Step 2: Wait for operator to be ready (this creates the required CRDs)
oc wait --for=condition=Ready pod -l app.kubernetes.io/name=lightspeed-operator -n openshift-lightspeed --timeout=300s

# ğŸ¯ Step 3: Apply complete configuration
oc apply -k k8s/lightspeed/overlays/qwen3-4b
```

### **What You Get**
- **AI Assistant** integrated into OpenShift console
- **YAML Generation** - "Create a deployment for nginx"  
- **Troubleshooting** - "Why is my pod failing?"
- **Best Practices** - Expert OpenShift guidance
- **Auto-discovery** - Automatically connects to your deployed models

### **Verification**
```bash
# âœ… Check all components are running
oc get pods -n openshift-lightspeed
oc get olsconfig cluster -n openshift-lightspeed
```

[ğŸ“– **Detailed Lightspeed Setup Guide**](k8s/lightspeed/README.md)

## ğŸ—ï¸ **Architecture Overview**

```mermaid
graph TB
    subgraph "ğŸš€ GitOps Layer"
        G[ğŸ“ Git Repository]
        A[ğŸ”„ ArgoCD/OpenShift GitOps]
    end
    
    subgraph "â˜¸ï¸ Kubernetes Cluster"
        subgraph "ğŸ¤– ramalama namespace"
            M1[ğŸ“¦ qwen3-1b-deployment]
            M2[ğŸ“¦ qwen3-4b-deployment] 
            M3[ğŸ“¦ qwen3-30b-deployment]
            M4[ğŸ“¦ deepseek-r1-deployment]
        end
        
        subgraph "ğŸ¯ openshift-lightspeed namespace"
            LS[ğŸ¤– Lightspeed Assistant]
        end
    end
    
    subgraph "ğŸ‘¨â€ğŸ’» User Interfaces"
        CLI[ğŸ–¥ï¸ kubectl/oc CLI]
        WEB[ğŸŒ OpenShift Console]
        API[ğŸ”Œ REST APIs]
    end
    
    G --> A
    A --> M1
    A --> M2  
    A --> M3
    A --> M4
    A --> LS
    
    LS -.->|ğŸ”— Connects to| M2
    LS -.->|ğŸ”— Connects to| M3
    LS -.->|ğŸ”— Connects to| M4
    
    CLI --> M1
    CLI --> M2
    WEB --> LS
    API --> M1
    API --> M2
    
    style G fill:#e1f5fe
    style A fill:#f3e5f5
    style M1 fill:#e8f5e8
    style M2 fill:#e8f5e8
    style M3 fill:#e8f5e8  
    style M4 fill:#e8f5e8
    style LS fill:#fff3e0
```

## **Model Management**

### **Add New Models**

![Add Model](https://img.shields.io/badge/Script-add--model.sh-blue?logo=gnu-bash)

```bash
# ğŸ¯ Interactive mode
./scripts/add-model.sh --interactive

# ğŸš€ Command line mode with Lightspeed generation
./scripts/add-model.sh \
  --name "llama-7b" \
  --description "Llama 7B Chat model" \
  --model-gguf-url "hf://microsoft/Llama-7b-gguf" \
  --model-file "/mnt/models/llama-7b.gguf/llama-7b.gguf" \
  --create-lightspeed-overlay
```

**Auto-Generated Files:**
- ğŸ“¦ `containerfiles/Containerfile-llama-7b`
- â˜¸ï¸ `k8s/models/llama-7b/kustomization.yaml`
- ğŸ¯ `k8s/lightspeed/overlays/llama-7b/kustomization.yaml`
- ğŸ¤– `k8s/lightspeed/overlays/llama-7b/olsconfig.yaml`
- ğŸ“– `k8s/lightspeed/overlays/llama-7b/README.md`
- âš™ï¸ `models/llama-7b.conf`

### **List Models**
```bash
./scripts/list-models.sh
```

### **Remove Models**
```bash
./scripts/remove-model.sh llama-7b --force
```

### **Deployment Order (Important!)**

When deploying both models and OpenShift Lightspeed:

```bash
# 1ï¸âƒ£ Deploy model first
oc apply -f k8s/models/ramalama-namespace.yaml
oc apply -k k8s/models/llama-7b

# 2ï¸âƒ£ Wait for model to be ready
oc wait --for=condition=Ready pod -l model=llama-7b -n ramalama --timeout=300s

# 3ï¸âƒ£ Then deploy Lightspeed (two-step process)
oc apply -k k8s/lightspeed/base/operator-only
oc wait --for=condition=Ready pod -l app.kubernetes.io/name=lightspeed-operator -n openshift-lightspeed --timeout=100s
oc apply -k k8s/lightspeed/overlays/llama-7b
```

> [!IMPORTANT]  
> **Model First, Then Lightspeed**: Always deploy your models before deploying OpenShift Lightspeed to ensure proper service discovery and connectivity.

## **Production-like Deployment**

### **With OpenShift GitOps**

![GitOps](https://img.shields.io/badge/GitOps-Enabled-green?logo=argo)

```bash
# ğŸ”„ Single model application
oc apply -f k8s/argocd/application-example.yaml

# ğŸŒ Multi-environment ApplicationSet
oc apply -f k8s/argocd/applicationset-example.yaml
```

### ğŸ“Š **Resource Requirements**

| Model Size | Memory | CPU | GPU | Storage |
|------------|--------|-----|-----|---------|
| **1.7B** | 4Gi | 2 cores | Optional | 10Gi |
| **4B** | 8Gi | 4 cores | Optional | 20Gi |
| **8B** | 16Gi | 8 cores | Recommended | 40Gi |
| **30B** | 32Gi | 16 cores | Required | 80Gi |

### ğŸ” **Security Features**

- **Pod Security Standards** - Restricted policies enforced
- **Non-root execution** - All containers run as non-root user
- **RBAC** - Role-based access control
- **Network Policies** - Micro-segmentation ready
- **Security Context** - Dropped capabilities and seccomp

## ğŸ“ **Project Structure**

```
ramalama-k8s/
â”œâ”€â”€ ğŸ“¦ containerfiles/           # Container build definitions
â”œâ”€â”€ â˜¸ï¸ k8s/                      # Kubernetes manifests
â”‚   â”œâ”€â”€ ğŸ—ï¸ base/                 # Base Kustomize resources  
â”‚   â”œâ”€â”€ ğŸ­ models/               # Model-specific configs
â”‚   â”œâ”€â”€ ğŸŒ overlays/             # Environment overlays
â”‚   â”œâ”€â”€ ğŸ¯ lightspeed/           # OpenShift Lightspeed integration
â”‚   â””â”€â”€ ğŸ”„ argocd/               # GitOps applications
â”œâ”€â”€ ğŸ¤– models/                   # Model configurations
â”œâ”€â”€ ğŸ› ï¸ scripts/                  # Management scripts
â””â”€â”€ ğŸ“š docs/                     # Documentation
```

## **Configuration**

### **Model Parameters**
```yaml
# Example model configuration
configMapGenerator:
- name: ramalama-config
  literals:
  - CTX_SIZE=20048        # ğŸ§  Context window size
  - THREADS=14            # ğŸ”„ CPU threads
  - TEMP=0.7              # ğŸŒ¡ï¸ Temperature (creativity)
  - TOP_K=40              # ğŸ¯ Top-K sampling
  - TOP_P=0.9             # ğŸ“Š Top-P sampling
```

### **Environment Variables**
```bash
# ğŸšª API Configuration
PORT=8080
HOST=0.0.0.0

# ğŸ§  Model Settings  
MODEL_FILE=/mnt/models/model.gguf
CTX_SIZE=4096
THREADS=14

# ï¿½ï¿½ Server Options
LOG_COLORS=true
NO_WARMUP=false
JINJA=true
```

## **Monitoring & Troubleshooting**

### **Health Checks**
```bash
# âœ… Check model health
oc get pods -l app.kubernetes.io/name=ramalama -n ramalama

# ğŸ“ View logs
oc logs -l model=qwen3-4b -n ramalama --tail=100

# ğŸ”§ Debug service connectivity
oc port-forward -n ramalama svc/qwen3-4b-ramalama-service 8080:8080
curl http://localhost:8080/v1/models
```

### ğŸ†˜ **Common Issues**

| Issue | Solution |
|-------|----------|
| ğŸš« Pod not starting | Check resource limits and node capacity |
| ğŸ”Œ API not responding | Verify port-forward and service endpoints |  
| ğŸŒ Slow responses | Increase CPU/memory or enable GPU |
| ğŸ“¦ Image pull errors | Check registry credentials and image tags |

## ğŸ¤ **Contributing**

![Contributors](https://img.shields.io/badge/Contributors-Welcome-brightgreen?logo=github)

1. **ğŸ´ Fork** the repository
2. **ğŸŒ¿ Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **ğŸ’¾ Commit** your changes (`git commit -m 'Add amazing feature'`)
4. **ğŸ“¤ Push** to the branch (`git push origin feature/amazing-feature`)
5. **ğŸ”„ Open** a Pull Request

### **Development Setup**
```bash
# ğŸ  Local development
./scripts/build-script.sh
podman build -f containerfiles/Containerfile-min .

# ğŸ§ª Test locally
./scripts/llama-server.sh
```

## ğŸ“š **Documentation**

- **[Kubernetes Deployment Guide](k8s/README.md)**
- **[OpenShift Lightspeed Integration](k8s/lightspeed/README.md)**  
- **[Model Management](MODELS.md)**
- **[GitOps Setup](OPENSHIFT_GITOPS.md)**

## ğŸ“œ **License**

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™‹ **Support**

- **ğŸ› Issues**: [GitHub Issues](https://github.com/kush-gupt/ramalama-k8s/issues)
- **ğŸ’¬ Discussions**: [GitHub Discussions](https://github.com/kush-gupt/ramalama-k8s/discussions)
---

**ğŸ‰ Ready to deploy your own AI models?**

[![Get Started](https://img.shields.io/badge/Get%20Started-Now-success?style=for-the-badge&logo=rocket)](k8s/README.md)
[![OpenShift Lightspeed](https://img.shields.io/badge/OpenShift%20Lightspeed-Deploy-red?style=for-the-badge&logo=redhatopenshift)](k8s/lightspeed/README.md)

*Made for the open source community*
