# Ramalama Models Configuration
# This file contains the configuration for all models in the repository

models:
  gpt-oss-20b:
    name: "GPT-OSS 20B"
    description: "All-in-one Ramalama server with embedded GPT-OSS 20B Q4_K_XL.gguf Unsloth model."
    model_source: "gpt-oss-20b-source"
    model_gguf_url: "hf://unsloth/gpt-oss-20b-GGUF/gpt-oss-20b-UD-Q4_K_XL.gguf"
    model_file: "/mnt/models/gpt-oss-20b-UD-Q4_K_XL.gguf/gpt-oss-20b-UD-Q4_K_XL.gguf"
    maintainer: "Kush Gupta"
    create_lightspeed_overlay: true
    lightspeed_namespace: "ramalama"
    parameters:
      ctx_size: 20048
      threads: -1
      temp: 1.0
      top_k: 0
      top_p: 1.0
      cache_reuse: 256

  qwen3-1b:
    name: "Qwen 3 1.7B"
    description: "All-in-one Ramalama server with embedded Qwen3-1.7B Q4_K_XL.gguf Unsloth model."
    model_source: "qwen3-1b-source"
    model_gguf_url: "hf://unsloth/Qwen3-1.7B-GGUF/Qwen3-1.7B-UD-Q4_K_XL.gguf"
    model_file: "/mnt/models/Qwen3-1.7B-UD-Q4_K_XL.gguf/Qwen3-1.7B-UD-Q4_K_XL.gguf"
    maintainer: "Kush Gupta"
    create_lightspeed_overlay: true
    lightspeed_namespace: "ramalama"
    parameters:
      ctx_size: 32000
      threads: 16
      temp: 0.6
      top_k: 20
      top_p: 0.95
      cache_reuse: 256
    resources:
      requests:
        memory: "4Gi"
        cpu: "2"

  qwen3-4b:
    name: "Qwen 3 4B"
    description: "All-in-one Ramalama server with embedded Qwen3-4B Q4_K_M.gguf Unsloth model."
    model_source: "qwen-4b-source"
    model_gguf_url: "hf://unsloth/Qwen3-4B-GGUF/Qwen3-4B-Q4_K_M.gguf"
    model_file: "/mnt/models/Qwen3-4B-Q4_K_M.gguf/Qwen3-4B-Q4_K_M.gguf"
    maintainer: "Kush Gupta"
    parameters:
      ctx_size: 20048
      threads: 14
      temp: 0.6
      top_k: 20
      top_p: 0.95
      cache_reuse: 256
    resources:
      requests:
        memory: "4Gi"
        cpu: "2"

  qwen3-30b:
    name: "Qwen 3 30B"
    description: "All-in-one Ramalama server with embedded Qwen3-30B-A3B-UD-Q4_K_XL.gguf Unsloth model."
    model_source: "qwen-30b-source"
    model_gguf_url: "hf://unsloth/Qwen3-30B-A3B-GGUF/Qwen3-30B-A3B-UD-Q4_K_XL.gguf"
    model_file: "/mnt/models/Qwen3-30B-A3B-UD-Q4_K_XL.gguf/Qwen3-30B-A3B-UD-Q4_K_XL.gguf"
    maintainer: "Kush Gupta"
    parameters:
      ctx_size: 20048
      threads: 14
      temp: 0.7
      top_k: 20
      top_p: 0.8
      cache_reuse: 256
    resources:
      requests:
        memory: "16Gi"
        cpu: "4"

  qwen3-30b-a3b-instruct-2507:
    name: "Qwen 3 30B A3B Instruct 2507"
    description: "All-in-one Ramalama server with embedded Qwen3-30B-A3B-Instruct-2507-UD-Q3_K_XL.gguf Unsloth model."
    model_source: "qwen3-30b-a3b-instruct-2507-source"
    model_gguf_url: "hf://unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF/Qwen3-30B-A3B-Instruct-2507-UD-Q3_K_XL.gguf"
    model_file: "/mnt/models/Qwen3-30B-A3B-Instruct-2507-UD-Q3_K_XL.gguf/Qwen3-30B-A3B-Instruct-2507-UD-Q3_K_XL.gguf"
    maintainer: "Kush Gupta"
    create_lightspeed_overlay: true
    lightspeed_namespace: "ramalama"
    parameters:
      ctx_size: 20048
      threads: 16
      temp: 0.7
      top_k: 20
      top_p: 0.8
      cache_reuse: 256
    resources:
      requests:
        memory: "16Gi"
        cpu: "4"

  deepseek-r1-qwen3-8b:
    name: "DeepSeek R1 Qwen3 8B"
    description: "All-in-one Ramalama server with embedded Unsloth DeepSeek-R1-0528-Qwen3-8B-UD-Q4_K_XL.gguf"
    model_source: "deepseek-r1-qwen3-8b-source"
    model_gguf_url: "hf://unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/DeepSeek-R1-0528-Qwen3-8B-UD-Q4_K_XL.gguf"
    model_file: "/mnt/models/DeepSeek-R1-0528-Qwen3-8B-UD-Q4_K_XL.gguf/DeepSeek-R1-0528-Qwen3-8B-UD-Q4_K_XL.gguf"
    maintainer: "Kush Gupta"
    parameters:
      ctx_size: 20048
      threads: 14
      temp: 0.6
      top_k: 20
      top_p: 0.95
      cache_reuse: 256
    resources:
      requests:
        memory: "16Gi"
        cpu: "4"

  gemma-3n-e4b:
    name: "Gemma 3N E4B"
    description: "All-in-one Ramalama server with embedded Gemma 3N E4B Q4_K_XL.gguf Unsloth model."
    model_source: "gemma-3n-e4b-source"
    model_gguf_url: "hf://unsloth/gemma-3n-E4B-it-GGUF/gemma-3n-E4B-it-UD-Q4_K_XL.gguf"
    model_file: "/mnt/models/gemma-3n-E4B-it-UD-Q4_K_XL.gguf/gemma-3n-E4B-it-UD-Q4_K_XL.gguf"
    maintainer: "Kush Gupta"
    create_lightspeed_overlay: true
    lightspeed_namespace: "ramalama"
    parameters:
      ctx_size: 20048
      threads: 14
      temp: 1.0
      top_k: 64
      top_p: 0.95
      cache_reuse: 256
    resources:
      requests:
        memory: "4Gi"
        cpu: "2"
# Template configurations for common model families
  gemma-3-12b:
    name: "gemma-3-12b"
    description: "gemma-3-12b-it-Q4_K_M.gguf from Unsloth"
    model_source: "gemma-3-12b-source"
    model_gguf_url: "hf://unsloth/gemma-3-12b-it-GGUF/gemma-3-12b-it-Q4_K_M.gguf"
    model_file: "/mnt/models/gemma-3-12b-it-Q4_K_M.gguf/gemma-3-12b-it-Q4_K_M.gguf"
    maintainer: "Kush Gupta"
    create_lightspeed_overlay: true
    lightspeed_namespace: "ramalama"
    parameters:
      ctx_size: 20048
      threads: 14
      temp: 0.6
      top_k: 20
      top_p: 0.95
      cache_reuse: 256
templates:
  llama:
    parameters:
      ctx_size: 4096
      threads: 14
      temp: 0.7
      top_k: 40
      top_p: 0.9
      cache_reuse: 256
    resources:
      small: # <7B parameters
        requests:
          memory: "4Gi"
          cpu: "2"
        limits:
          memory: "8Gi"
          cpu: "4"
      medium: # 7B-13B parameters
        requests:
          memory: "8Gi"
          cpu: "4"
        limits:
          memory: "16Gi"
          cpu: "6"
      large: # >13B parameters
        requests:
          memory: "16Gi"
          cpu: "6"
        limits:
          memory: "32Gi"
          cpu: "8"

  mistral:
    parameters:
      ctx_size: 8192
      threads: 14
      temp: 0.6
      top_k: 50
      top_p: 0.95
      cache_reuse: 256
    resources:
      small:
        requests:
          memory: "4Gi"
          cpu: "2"
        limits:
          memory: "8Gi"
          cpu: "4"
      medium:
        requests:
          memory: "8Gi"
          cpu: "4"
        limits:
          memory: "16Gi"
          cpu: "6"

# Global defaults
defaults:
  maintainer: "Kush Gupta"
  parameters:
    ctx_size: 4096
    threads: 14
    temp: 0.7
    top_k: 40
    top_p: 0.9
    min_p: 0
    cache_reuse: 256
    host: "0.0.0.0"
    port: 8080
  resources:
    requests:
      memory: "4Gi"
      cpu: "2"
    limits:
      memory: "8Gi"
      cpu: "4"
  labels:
    version: "latest"
    component: "llm-server" 
